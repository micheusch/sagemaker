{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quick_text_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWbmcUBjNF0k68AeeliSVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micheusch/sagemaker/blob/main/quick_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WOJDWyLw539"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "# # import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv('drive/MyDrive/Colab Notebooks/data/disaster_tweets.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "DUWcIxnBw8C6",
        "outputId": "5b4c7c6d-f28a-49c4-eaed-7f68661c5690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-40a65c1a-9fad-4402-9614-4d794c8430fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40a65c1a-9fad-4402-9614-4d794c8430fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40a65c1a-9fad-4402-9614-4d794c8430fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40a65c1a-9fad-4402-9614-4d794c8430fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btjIO3MOyi-V",
        "outputId": "64ea9ef6-4637-4f07-ddb2-9b71a62ff910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        7613 non-null   int64 \n",
            " 1   keyword   7552 non-null   object\n",
            " 2   location  5080 non-null   object\n",
            " 3   text      7613 non-null   object\n",
            " 4   target    7613 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 297.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfUaOH46yjnm",
        "outputId": "1d56a6fa-b933-428b-f215-c1e772a1efd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id          7613\n",
              "keyword      221\n",
              "location    3341\n",
              "text        7503\n",
              "target         2\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Data prep"
      ],
      "metadata": {
        "id": "1TQr-Tzay3Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "def remove_at_hash(sent):\n",
        "    \"\"\" Returns a string with @-symbols and hashtags removed. \"\"\"\n",
        "    return re.sub(r'@|#', r'', sent.lower())\n",
        "\n",
        "def remove_sites(sent):\n",
        "    \"\"\" Returns a string with any websites starting with 'http.' removed. \"\"\"\n",
        "    return re.sub(r'http.*', r'', sent.lower())\n",
        "\n",
        "def remove_punct(sent):\n",
        "    \"\"\" Returns a string with only English unicode word characters ([a-zA-Z0-9_]). \"\"\"\n",
        "    return ' '.join(re.findall(r'\\w+', sent.lower()))\n",
        "\n",
        "def spacy_cleaning(doc):\n",
        "    \"\"\" Returns a string that has been lemmatized and rid of stop words via SpaCy. \"\"\"\n",
        "    doc = nlp(doc.lower())\n",
        "    text = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "3rFoClcMys0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en', disable=['ner', 'parser'])\n",
        "\n",
        "\n",
        "data['text_simple'] = data['text'].apply(lambda x: remove_punct(remove_sites(remove_at_hash(x))))\n",
        "data['text_spacy'] = data['text'].apply(lambda x: spacy_cleaning(x))"
      ],
      "metadata": {
        "id": "6S8GgLIJzN3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### n-grams"
      ],
      "metadata": {
        "id": "wAuzz97b0D9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "text = [re.split('\\s+', tweet) for tweet in data['text']]\n",
        "bigram_phrases = Phrases(text, min_count=30)\n",
        "bigram = Phraser(bigram_phrases)\n",
        "bigram_text = bigram[text]\n",
        "\n",
        "trigram_phrases = Phrases(bigram_text, min_count=30)\n",
        "trigram = Phraser(trigram_phrases)\n",
        "trigram_text = trigram[bigram_text]\n",
        "\n",
        "data['text_trigram'] = [' '.join(tweet) for tweet in trigram_text]\n",
        "\n",
        "text_simple = [re.split('\\s+', tweet) for tweet in data['text_simple']]\n",
        "\n",
        "bigram_phrases = Phrases(text_simple, min_count=30)\n",
        "bigram = Phraser(bigram_phrases)\n",
        "bigram_text_simple = bigram[text_simple]\n",
        "\n",
        "trigram_phrases = Phrases(bigram_text_simple, min_count=30)\n",
        "trigram = Phraser(trigram_phrases)\n",
        "trigram_text_simple = trigram[bigram_text_simple]\n",
        "\n",
        "data['text_trigram_simple'] = [' '.join(tweet) for tweet in trigram_text_simple]\n",
        "\n",
        "text_spacy = [re.split('\\s+', tweet) for tweet in data['text_spacy']]\n",
        "\n",
        "bigram_phrases = Phrases(text_spacy, min_count=30)\n",
        "bigram = Phraser(bigram_phrases)\n",
        "bigram_text_spacy = bigram[text_spacy]\n",
        "\n",
        "trigram_phrases = Phrases(bigram_text_spacy, min_count=30)\n",
        "trigram = Phraser(trigram_phrases)\n",
        "trigram_text_spacy = trigram[bigram_text_spacy]\n",
        "\n",
        "data['text_trigram_spacy'] = [' '.join(tweet) for tweet in trigram_text_spacy]"
      ],
      "metadata": {
        "id": "5OQI9QH9zPRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train/test split"
      ],
      "metadata": {
        "id": "lsT0-TlE1aQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, valid = train_test_split(data, random_state=24)\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "valid = valid.reset_index(drop=True)\n",
        "train[['text','target']].rename(columns={'target':'label'}).to_csv('drive/MyDrive/Colab Notebooks/data/disaster_tweets_train.csv', index=False)\n",
        "valid[['text','target']].rename(columns={'target':'label'}).to_csv('drive/MyDrive/Colab Notebooks/data/disaster_tweets_val.csv', index=False)\n",
        "\n",
        "train.shape, valid.shape, data.shape\n",
        "\n",
        "disasters = train[train['target'] == 1].reset_index()\n",
        "not_disasters = train[train['target'] == 0].reset_index()"
      ],
      "metadata": {
        "id": "eWKvKT0z1RoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disasters.nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GNUrqMr1sRK",
        "outputId": "ce89d9e8-6339-449c-d6ae-a0417db48bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "level_0                2450\n",
              "index                  2450\n",
              "id                     2450\n",
              "keyword                 220\n",
              "location               1197\n",
              "text                   2418\n",
              "target                    1\n",
              "text_simple            2130\n",
              "text_spacy             2417\n",
              "text_trigram           2417\n",
              "text_trigram_simple    2130\n",
              "text_trigram_spacy     2416\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "not_disasters.nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CODuiyBx14Ah",
        "outputId": "5fa7b34b-e8a3-483a-e866-50c142fe4969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "level_0                3259\n",
              "index                  3259\n",
              "id                     3259\n",
              "keyword                 216\n",
              "location               1668\n",
              "text                   3239\n",
              "target                    1\n",
              "text_simple            3069\n",
              "text_spacy             3238\n",
              "text_trigram           3239\n",
              "text_trigram_simple    3069\n",
              "text_trigram_spacy     3238\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF-IDF"
      ],
      "metadata": {
        "id": "wmpQdaVC14yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.similarities import MatrixSimilarity\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "vma2i-oD2ItD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disaster_tweets = disasters['text'].tolist()\n",
        "not_disaster_tweets = not_disasters['text'].tolist()\n",
        "\n",
        "disaster_tweets_split = [\n",
        "    [word for word in tweet.split()]\n",
        "    for tweet in disaster_tweets\n",
        "]\n",
        "not_disaster_tweets_split = [\n",
        "    [word for word in tweet.split()]\n",
        "    for tweet in not_disaster_tweets\n",
        "]"
      ],
      "metadata": {
        "id": "8GIQ1SYL2Y02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disaster_tweets_word_frequency = defaultdict(int)\n",
        "for tweet in disaster_tweets_split:\n",
        "    for word in tweet:\n",
        "        disaster_tweets_word_frequency[word] += 1\n",
        "        \n",
        "not_disaster_tweets_word_frequency = defaultdict(int)\n",
        "for tweet in not_disaster_tweets_split:\n",
        "    for word in tweet:\n",
        "        not_disaster_tweets_word_frequency[word] += 1\n",
        "\n",
        "disaster_tweets_split = [\n",
        "    [word for word in tweet if disaster_tweets_word_frequency[word] > 1]\n",
        "    for tweet in disaster_tweets_split\n",
        "]\n",
        "\n",
        "not_disaster_tweets_split = [\n",
        "    [word for word in tweet if not_disaster_tweets_word_frequency[word] > 1]\n",
        "    for tweet in not_disaster_tweets_split\n",
        "]"
      ],
      "metadata": {
        "id": "wGzI3erF2xwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disaster_tweets_dct = Dictionary(disaster_tweets_split)\n",
        "not_disaster_tweets_dct = Dictionary(not_disaster_tweets_split)\n",
        "\n",
        "disaster_tweets_corpus = [disaster_tweets_dct.doc2bow(tweet) for tweet in disaster_tweets_split]\n",
        "not_disaster_tweets_corpus = [not_disaster_tweets_dct.doc2bow(tweet) for tweet in not_disaster_tweets_split]"
      ],
      "metadata": {
        "id": "-oSGKJIe240P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disaster_tweets_tfidf = TfidfModel(disaster_tweets_corpus)\n",
        "not_disaster_tweets_tfidf = TfidfModel(not_disaster_tweets_corpus)\n",
        "\n",
        "disaster_tweets_tfidf_vectors = disaster_tweets_tfidf[disaster_tweets_corpus]\n",
        "not_disaster_tweets_tfidf_vectors = not_disaster_tweets_tfidf[not_disaster_tweets_corpus]\n",
        "\n",
        "disaster_tweets_similarity = MatrixSimilarity(disaster_tweets_tfidf_vectors)\n",
        "not_disaster_tweets_similarity = MatrixSimilarity(not_disaster_tweets_tfidf_vectors)"
      ],
      "metadata": {
        "id": "OQwWxwP_4chj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_tweets = valid['text'].tolist()\n",
        "\n",
        "valid_tweets_split = [\n",
        "    [word for word in tweet.split()]\n",
        "    for tweet in valid_tweets\n",
        "]\n",
        "\n",
        "valid_tweets_word_frequency = defaultdict(int)\n",
        "for tweet in valid_tweets_split:\n",
        "    for word in tweet:\n",
        "        valid_tweets_word_frequency[word] += 1\n",
        "    \n",
        "valid_tweets_split = [\n",
        "    [word for word in tweet if valid_tweets_word_frequency[word] > 1]\n",
        "    for tweet in valid_tweets_split\n",
        "]"
      ],
      "metadata": {
        "id": "RAylPYSn5Jwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid['prediction'] = np.zeros(len(valid)).astype('int')"
      ],
      "metadata": {
        "id": "vfyWlqfz7K2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in range(len(valid)):\n",
        "    tweet = valid_tweets_split[row]\n",
        "    \n",
        "    tweet_bow_with_disasters_dct = disaster_tweets_dct.doc2bow(tweet)\n",
        "    tweet_bow_with_not_disasters_dct = not_disaster_tweets_dct.doc2bow(tweet)\n",
        "    \n",
        "    tweet_tfidf_vector_with_disasters_tfidf = disaster_tweets_tfidf[tweet_bow_with_disasters_dct]\n",
        "    tweet_tfidf_vector_with_not_disasters_tfidf = not_disaster_tweets_tfidf[tweet_bow_with_not_disasters_dct]\n",
        "    \n",
        "    disaster_similarity_vector = disaster_tweets_similarity[tweet_tfidf_vector_with_disasters_tfidf]\n",
        "    not_disaster_similarity_vector = not_disaster_tweets_similarity[tweet_tfidf_vector_with_not_disasters_tfidf]\n",
        "    \n",
        "    disaster_tally = np.where(disaster_similarity_vector > 0.1)[0].size # np.where() returns a tuple, so we have to index into [0] to get what we want\n",
        "    not_disaster_tally = np.where(not_disaster_similarity_vector > 0.1)[0].size\n",
        "    \n",
        "    if disaster_tally > not_disaster_tally:\n",
        "        valid.loc[row, 'prediction'] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "m1IVGYz97OMQ",
        "outputId": "8b2c625f-c98a-4f6b-c287-81aed0bbed56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-88c7d9e006ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdisaster_similarity_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisaster_tweets_similarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet_tfidf_vector_with_disasters_tfidf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnot_disaster_similarity_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnot_disaster_tweets_similarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet_tfidf_vector_with_not_disasters_tfidf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdisaster_tally\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisaster_similarity_vector\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;31m# np.where() returns a tuple, so we have to index into [0] to get what we want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_best\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/similarities/docsim.py\u001b[0m in \u001b[0;36mget_similarities\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;31m# do a little transposition dance to stop numpy from making a copy of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# self.index internally in numpy.dot (very slow).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[0;31m# return #queries x #index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m  \u001b[0;31m# XXX: removed casting the result from array to list; does anyone care?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(valid['target'], valid['prediction'])\n",
        "F1 = f1_score(valid['target'], valid['prediction'])\n",
        "accuracy, F1"
      ],
      "metadata": {
        "id": "5NECIeS57nWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word2Vec"
      ],
      "metadata": {
        "id": "unVEi6_D7sTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#collapse\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import time"
      ],
      "metadata": {
        "id": "S1SEMzGN77Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collapse\n",
        "def replace_unknowns(search_texts, min_count):\n",
        "    \"\"\"\n",
        "    Replaces words that occur less than a certain number of times\n",
        "    in a string or list of strings with 'UNK'.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    search_texts : list\n",
        "        A list of input strings to iterate over.\n",
        "    min_count : int\n",
        "        An integer specify the minimum count a word should occur in\n",
        "        the search_texts to not be replaced with 'UNK'.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        List of search_texts with words that occur less than the min_count\n",
        "        amount of times replaced with 'UNK'.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # Get all tweets lowered and tokenized.\n",
        "    # This makes sense because we'd never want to\n",
        "    # treat an 'a' different from an 'A'.\n",
        "    # (Capitalization is just an orthographical convention)\n",
        "    texts = [\n",
        "        [word for word in re.split('\\s+', text.lower())]\n",
        "        for text in search_texts\n",
        "    ]\n",
        "\n",
        "    # create a dictionary that stores the count of each\n",
        "    # word in our uncleaned tweets. We can insert new words\n",
        "    # into the dict or add to their count if their already in it.\n",
        "    vocab_counts = defaultdict(int)\n",
        "\n",
        "    # Create a list that we can append words that occur more than\n",
        "    # the desired threshold number of times to.\n",
        "    vocab = []\n",
        "\n",
        "    for text in texts:\n",
        "        for word in text:\n",
        "            vocab_counts[word] += 1\n",
        "\n",
        "    # Now go through the vocab_counts and get rid of\n",
        "    # words that occur less than five times.\n",
        "    for word in vocab_counts.keys():\n",
        "        if vocab_counts[word] > min_count:\n",
        "            vocab.append(word)\n",
        "\n",
        "    # Now initialize a new column in data that will hold\n",
        "    # the tweets with 'UNK' replacing words that occur\n",
        "    # across the entire vocabulary less than five times.\n",
        "    # This creates congruency later on in the model.\n",
        "    # data['text_count_5'] = np.empty(len(data), dtype=str) # ***** DO THIS OUTSIDE FUNC IN WORD2VEC SECTION\n",
        "\n",
        "    # Now, go through each tweet and replace the words that\n",
        "    # occur less than 5 times throughout the entire corpus\n",
        "    # with 'UNK'. Then, we insert the new tweet into a new\n",
        "    # column in the original dataframe.\n",
        "\n",
        "    out = []\n",
        "    # this process takes about a minute\n",
        "    for i, text in enumerate(texts):\n",
        "        text_replaced = []\n",
        "        for word in text:\n",
        "            if word in vocab:\n",
        "                text_replaced.append(word)\n",
        "            else:\n",
        "                text_replaced.append('UNK')\n",
        "        text_replaced = ' '.join(text_replaced)\n",
        "        out.append(text_replaced)\n",
        "        \n",
        "    return out"
      ],
      "metadata": {
        "id": "569T1ouU9GkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collapse\n",
        "valid['text_count_5'] = replace_unknowns(valid['text_trigram'], 5)\n",
        "valid['text_simple_5'] = replace_unknowns(valid['text_trigram_simple'], 5)\n",
        "valid['text_spacy_5'] = replace_unknowns(valid['text_trigram_spacy'], 5)\n",
        "\n",
        "disasters['text_count_5'] = replace_unknowns(disasters['text_trigram'], 5)\n",
        "disasters['text_simple_5'] = replace_unknowns(disasters['text_trigram_simple'], 5)\n",
        "disasters['text_spacy_5'] = replace_unknowns(disasters['text_trigram_spacy'], 5)\n",
        "\n",
        "not_disasters['text_count_5'] = replace_unknowns(not_disasters['text_trigram'], 5)\n",
        "not_disasters['text_simple_5'] = replace_unknowns(not_disasters['text_trigram_simple'], 5)\n",
        "not_disasters['text_spacy_5'] = replace_unknowns(not_disasters['text_trigram_spacy'], 5)"
      ],
      "metadata": {
        "id": "x5Z0wIkd9Kj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_count_5'] = replace_unknowns(data['text_trigram'], 5)\n",
        "data['text_simple_5'] = replace_unknowns(data['text_trigram_simple'], 5)\n",
        "data['text_spacy_5'] = replace_unknowns(data['text_trigram_spacy'], 5)\n"
      ],
      "metadata": {
        "id": "cRNX17_O_tVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = valid.drop(columns=['prediction'])"
      ],
      "metadata": {
        "id": "a2vfDQe49i1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(min_count=5, sample=1e-3, workers=4, seed=24)"
      ],
      "metadata": {
        "id": "3-Nzwqb-9k-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = [\n",
        "    [wd for wd in tweet.split(' ')]\n",
        "    for tweet in data['text_count_5']\n",
        "]\n",
        "\n",
        "model.build_vocab(tweets)\n",
        "model.train(tweets, total_examples=model.corpus_count, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvNAWiGx9uWG",
        "outputId": "b4f9f036-c69a-4ac9-b8f5-96194d875706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2007377, 3383040)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.init_sims(replace=True)"
      ],
      "metadata": {
        "id": "RHyNvSkv-BzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid['prediction'] = np.zeros(len(valid)).astype('int')"
      ],
      "metadata": {
        "id": "gqgyOgAd_48J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for valid_row in range(len(valid)):\n",
        "    valid_tweet = valid.loc[valid_row, 'text_count_5']\n",
        "    tokenized_valid_tweet = re.split('\\s+', valid_tweet) # split on all whitespace characters\n",
        "    \n",
        "    disaster_count = 0\n",
        "    not_disaster_count = 0\n",
        "    \n",
        "    # we can just reuse \"disasters\" and\n",
        "    # \"not_disasters\" from earlier!\n",
        "    for disaster_row in range(len(disasters)):\n",
        "        disaster_tweet = disasters.loc[disaster_row, 'text_count_5']\n",
        "        tokenized_disaster_tweet = re.split('\\s+', disaster_tweet)\n",
        "        if model.wv.n_similarity(tokenized_valid_tweet, tokenized_disaster_tweet) > 0.7:\n",
        "            disaster_count += 1\n",
        "        \n",
        "    for not_disaster_row in range(len(not_disasters)):\n",
        "        not_disaster_tweet = not_disasters.loc[not_disaster_row, 'text_count_5']\n",
        "        tokenized_not_disaster_tweet = re.split('\\s+', not_disaster_tweet)\n",
        "        if model.wv.n_similarity(tokenized_valid_tweet, tokenized_not_disaster_tweet) > 0.7:\n",
        "            not_disaster_count += 1\n",
        "            \n",
        "    if disaster_count > not_disaster_count:\n",
        "        valid.loc[valid_row, 'prediction'] = 1\n",
        "        \n",
        "end_time = time.time()\n",
        "print(f'Runtime: {(end_time - start_time) / 60.0} mins')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uYBKOHlA4in",
        "outputId": "f627fdd0-d406-40c9-b3f5-6ae96ddb15fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime: 23.706074607372283 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKG0KwgjA6rt",
        "outputId": "3fedac3b-a52d-4031-c911-9bd4e672ea30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0d202fb3-32b7-421b-9014-10e3e6602802\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_simple</th>\n",
              "      <th>text_spacy</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>text_trigram_simple</th>\n",
              "      <th>text_trigram_spacy</th>\n",
              "      <th>text_count_5</th>\n",
              "      <th>text_simple_5</th>\n",
              "      <th>text_spacy_5</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3068</td>\n",
              "      <td>4402</td>\n",
              "      <td>electrocute</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Kids got Disney version of the game Operation ...</td>\n",
              "      <td>0</td>\n",
              "      <td>kids got disney version of the game operation ...</td>\n",
              "      <td>kid get disney version game operation 2 aa bat...</td>\n",
              "      <td>Kids got Disney version of the game Operation ...</td>\n",
              "      <td>kids got disney version of the game operation ...</td>\n",
              "      <td>kid get disney version game operation 2 aa bat...</td>\n",
              "      <td>UNK got UNK UNK of the game UNK only 2 UNK UNK...</td>\n",
              "      <td>kids got UNK UNK of the game UNK only 2 UNK UN...</td>\n",
              "      <td>kid get UNK version game UNK 2 UNK UNK ? UNK o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3148</td>\n",
              "      <td>4522</td>\n",
              "      <td>emergency</td>\n",
              "      <td>Indianapolis, IN</td>\n",
              "      <td>UPDATE: Indiana State Police reopening I-65 ne...</td>\n",
              "      <td>1</td>\n",
              "      <td>update indiana state police reopening i 65 nea...</td>\n",
              "      <td>update : indiana state police reopen i-65 near...</td>\n",
              "      <td>UPDATE: Indiana State Police reopening I-65 ne...</td>\n",
              "      <td>update indiana state police reopening i 65 nea...</td>\n",
              "      <td>update : indiana state police reopen i-65 near...</td>\n",
              "      <td>update: UNK state police UNK UNK near UNK UNK ...</td>\n",
              "      <td>update UNK state police UNK i UNK near UNK UNK...</td>\n",
              "      <td>update : UNK state police UNK UNK near UNK UNK...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3139</td>\n",
              "      <td>4511</td>\n",
              "      <td>emergency</td>\n",
              "      <td>Phoenix</td>\n",
              "      <td>God forbid anyone in my family knows how to an...</td>\n",
              "      <td>0</td>\n",
              "      <td>god forbid anyone in my family knows how to an...</td>\n",
              "      <td>god forbid family know answer phone . need new...</td>\n",
              "      <td>God forbid anyone in my family knows how to an...</td>\n",
              "      <td>god forbid anyone in my family knows how to an...</td>\n",
              "      <td>god forbid family know answer phone . need new...</td>\n",
              "      <td>god UNK UNK in my family UNK how to UNK a UNK ...</td>\n",
              "      <td>god UNK UNK in my family UNK how to UNK a phon...</td>\n",
              "      <td>god UNK family know UNK phone . need new emerg...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7485</td>\n",
              "      <td>10707</td>\n",
              "      <td>wreck</td>\n",
              "      <td>Alabama, USA</td>\n",
              "      <td>First wreck today. So so glad me and mom are o...</td>\n",
              "      <td>0</td>\n",
              "      <td>first wreck today so so glad me and mom are ok...</td>\n",
              "      <td>wreck today . glad mom okay . lot bad . happy ...</td>\n",
              "      <td>First wreck today. So so glad me and mom are o...</td>\n",
              "      <td>first wreck today so so glad me and mom are ok...</td>\n",
              "      <td>wreck today . glad mom okay . lot bad . happy ...</td>\n",
              "      <td>first wreck UNK so so UNK me and UNK are UNK U...</td>\n",
              "      <td>first wreck today so so UNK me and UNK are UNK...</td>\n",
              "      <td>wreck today . UNK UNK UNK . lot bad . UNK UNK ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6023</td>\n",
              "      <td>8608</td>\n",
              "      <td>seismic</td>\n",
              "      <td>Somalia</td>\n",
              "      <td>Exploration takes seismic shift in Gabon to So...</td>\n",
              "      <td>0</td>\n",
              "      <td>exploration takes seismic shift in gabon to so...</td>\n",
              "      <td>exploration take seismic shift gabon somalia -...</td>\n",
              "      <td>Exploration takes seismic shift in Gabon to So...</td>\n",
              "      <td>exploration takes seismic shift in gabon to so...</td>\n",
              "      <td>exploration take seismic shift gabon somalia -...</td>\n",
              "      <td>UNK UNK seismic UNK in UNK to UNK - UNK UNK UN...</td>\n",
              "      <td>UNK UNK seismic UNK in UNK to UNK UNK UNK</td>\n",
              "      <td>UNK take seismic UNK UNK UNK - UNK ( UNK ) UNK...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d202fb3-32b7-421b-9014-10e3e6602802')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d202fb3-32b7-421b-9014-10e3e6602802 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d202fb3-32b7-421b-9014-10e3e6602802');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   index     id  ...                                       text_spacy_5 prediction\n",
              "0   3068   4402  ...  kid get UNK version game UNK 2 UNK UNK ? UNK o...          0\n",
              "1   3148   4522  ...  update : UNK state police UNK UNK near UNK UNK...          0\n",
              "2   3139   4511  ...  god UNK family know UNK phone . need new emerg...          0\n",
              "3   7485  10707  ...  wreck today . UNK UNK UNK . lot bad . UNK UNK ...          0\n",
              "4   6023   8608  ...  UNK take seismic UNK UNK UNK - UNK ( UNK ) UNK...          0\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(valid['target'], valid['prediction'])\n",
        "F1 = f1_score(valid['target'], valid['prediction'])\n",
        "accuracy, F1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWVat-KvA_WX",
        "outputId": "adc75633-f82f-4282-8514-13ce5ab21967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6313025210084033, 0.26875)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tranformers"
      ],
      "metadata": {
        "id": "_t0diy3u_WYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "jJYv-P_4LYvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "id": "HTsx853wBOP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "rpBGTp74NLk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "v8IFeIwIPBT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "# raw_datasets = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "8761oJJDO_FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "train_ds = load_dataset('csv', data_files='drive/MyDrive/Colab Notebooks/data/disaster_tweets_train.csv')\n",
        "valid_ds = load_dataset('csv', data_files='drive/MyDrive/Colab Notebooks/data/disaster_tweets_val.csv')"
      ],
      "metadata": {
        "id": "WWN-ihor89oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "small_train_dataset = train_ds.map(tokenize_function, batched=True)\n",
        "small_eval_dataset = valid_ds.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "VAa2wGdpNqkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf_train_dataset = train_ds.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
        "# tf_eval_dataset = valid_ds.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
        "tf_train_dataset = train_ds.with_format(\"tensorflow\")\n",
        "tf_eval_dataset  = valid_ds.with_format(\"tensorflow\")"
      ],
      "metadata": {
        "id": "WmBwazpoLUMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tf_train_ds = train_ds.map(tokenize_function, batched=True)['train']\n",
        "tf_eval_ds = valid_ds.map(tokenize_function, batched=True)['train']"
      ],
      "metadata": {
        "id": "kszqe8s8L6ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "id": "eTlSVqbOAXZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test_trainer\")"
      ],
      "metadata": {
        "id": "I_DAMuWTAbC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, \n",
        "                  train_dataset=tf_train_ds, \n",
        "                  eval_dataset=tf_eval_ds)\n"
      ],
      "metadata": {
        "id": "jRO8z8PcAdwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "XTsjBSVZAkh9",
        "outputId": "38027054-0102-4bfc-c07c-81cc2e5f3c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 5709\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2142\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1174' max='2142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1174/2142 14:54 < 12:18, 1.31 it/s, Epoch 1.64/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.470200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.384200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to test_trainer/checkpoint-500\n",
            "Configuration saved in test_trainer/checkpoint-500/config.json\n",
            "Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to test_trainer/checkpoint-1000\n",
            "Configuration saved in test_trainer/checkpoint-1000/config.json\n",
            "Model weights saved in test_trainer/checkpoint-1000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_step_end\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_step_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0meval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             )\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# A Callback can skip the return of `control` if it doesn't change it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mon_step_end\u001b[0;34m(self, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch}/{state.num_train_epochs}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mforce_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_force_next_update\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         )\n\u001b[1;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_force_next_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, value, force_update, comment)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_time_per_item\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted_remaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_time_per_item\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mupdate_bar\u001b[0;34m(self, value, comment)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"[{spaced_value}/{self.total} {format_time(self.elapsed_time)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"[{spaced_value}/{self.total} {format_time(self.elapsed_time)} < {format_time(self.predicted_remaining)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\", {1/self.average_time_per_item:.2f} it/s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"]\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mf\", {self.comment}]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "me66EY5SA1z0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}